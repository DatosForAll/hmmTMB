---
title: "hmmTMB technical guide"
author: "ThÃ©o Michelot, Richard Glennie"
date: "`r Sys.Date()`"
output: 
  html_document:
    number_sections: true
bibliography: refs.bib
vignette: >
  %\VignetteIndexEntry{hmmTMB technical guide}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
  number_sections: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  message = FALSE, error = FALSE, warning = FALSE,
  comment = NA
)
```

```{r load-package, echo = FALSE}
library(hmmTMB)
set.seed(342)
```

# Introduction

The package hmmTMB implements hidden Markov models with flexible formulations for the parameters of the hidden state process and parameters of the observation distributions, including linear, smooth or random effects of covariates. This document provides some technical background on the implementation, and is only intended to users who would like to get an idea of the inner workings of the package. For a more accessible introduction to the package, including example analyses with detailed code, see the other vignette (`hmmTMB user guide').

# Mathematical background

## Hidden Markov models

A hidden Markov model (HMM) is comprised of two random processes:

  - the state process $(S_t)$, defined as a $J$-state Markov chain, such that $S_t \ in \{ 1, 2, \dots, J \}$ at any time $t = 1, 2, \dots$;
  
  - the (possibly multivariate) observation process $(Z_t)$. At each time $t$, the observation $Z_t$ arises from one of $J$ probability distributions, determined by the value $S_t$ of the state process.

The state process is parameterised by an initial distribution $\pi^{(1)}= [\Pr(S_1 = 1), \dots, \Pr(S_1 = J)]$, and by a transition probability matrix
$$
\Gamma = 
\begin{pmatrix}
  \gamma_{11} & \gamma_{12} & \cdots & \gamma_{1J} \\
  \gamma_{21} & \gamma_{22} & \cdots & \gamma_{2J} \\
  \vdots & \vdots & \ddots & \vdots \\
  \gamma_{J1} & \gamma_{J2} & \cdots & \gamma_{JJ} \\
\end{pmatrix},
$$
where the $(i,j)$-th entry is the probability of a transition from state $i$ to state $j$, i.e.,
$$
\gamma_{ij} = \Pr(S_t = j \vert S_t = i).
$$

The observations are typically assumed to arise from some parametric distribution, with one set of parameters for each possible value of the state process, and we can write
$$
p(Z_t = z_t \vert S_t = j) = f_Z(z_t, \theta_j)
$$
where $f_Z$ is the assumed pdf for the observation, and $\theta_j$ is the vector of parameters for state $j$.

The main goal of hmmTMB is to offer the possibility to specify flexible models for the transition probabilities or the state-dependent observation parameters, including fixed and random effects of covariates, and smooth relationships between parameters and covariates using smoothing splines. Here, we only provide a very succinct introduction to HMMs to explain how this is done in the package; for a comprehensive description, see for example @zucchini2016.

## Basis-penaly smoothing splines

Flexible relationships between the model parameters described in the previous section and covariates can be defined using basis-penalty smoothing splines, similarly to generalized additive models (GAMs; @wood2017). In this framework, a parameter $\theta$ (either a transition probability or a parameter of the observation distribution) can be specified at time $t$ by
$$
h(\theta_t) = \beta_0 + f_1(x_{1t}) + f_2(x_{2t}) + \dots,
$$
where $h$ is a link function, $\beta_0$ is an intercept parameter, and the functions $f_1, f_2, \dots$ define the smooth relationships between $\theta_t$ and the covariates $x_{1t}, x_{2t}, \dots$. The smooth functions are written as linear combinations of basis functions,
$$
f_{i} (x) = \sum_{k=1}^K \beta_{ik} \psi_{ik}(x),
$$
where various bases can be chosen (e.g., cubic splines, B-splines). Following standard GAM methodology, the roughness (`wiggliness') of these functions is then penalised in the likelihood of the model to ensure that they are smooth. The penalised log-likelihood is
$$
l_p = \log(L) - \sum_i \lambda_i \beta_i^T S_i \beta_i,
$$
where $L$ is the unpenalised HMM likelihood (e.g.\ computed using the forward algorithm), the $\lambda_i$ are smoothness parameters, and $S_i$ is a known penalty matrix determined by the choice of basis.

The smoothness parameters $\lambda_i$ are not known, and need to be estimated jointly with other model parameters. In hmmTMB, we use the marginal likelihood method; i.e., we treat the basis coefficients as random effects, and we consider the marginal likelihood of the fixed effects $\alpha$ and smoothness parameters $\lambda$,
$$
L(\alpha, \lambda \vert z_1, \dots, z_n) = \int p(z_1, \dots, z_n \vert \alpha, \beta) p(\beta \vert \lambda) d\beta,
$$
where $p(z_1, \dots, z_n \ vert \theta, \beta)$ is the HMM likelihood, and $p(\beta \vert \lambda)$ is a multivariate normal pdf with mean zero and block-diagonal precision matrix with blocks $\lambda_i S_i$.


# Implementation using TMB and mgcv

## Model specification using mgcv

Design matrices, smoothing matrix, formula syntax...

## Model fitting using TMB

Laplace approximation. List of parameters, list of data, specification of fixed and random parameters...

Warning about Laplace approximation for multimodal distribution? Reference to Brett's preprint to justify that it's okay if the random effects are in the transition probabilities?

# Package structure

  - R6 classes
  
  - Code structure for each class
  
  - Implementation details: make_mat, par_all, obs_probs, coeff_fe, coeff_re, lambda...

## Observation distribution

Distributions for the observation process are created using the Dist class. To define a new distribution, we provide its name (as a string), its probability density/mass function (as a function), the random generator function, and the link functions for its parameters (as named lists of functions). For example, for the normal distribution, we have
```{r dist-def}
dist_norm <- Dist$new(
  name = "norm", 
  pdf = dnorm,
  rng = rnorm,
  link = list(mean = identity, sd = log),
  invlink = list(mean = identity, sd = exp),
  npar = 2
)
```

The Poisson, gamma, normal, beta, and von Mises distributions are included in the package, and more will be implemented in the future.

The functions n2w and w2n transform parameters of the observation distributions from the natural to the working scale and vice-versa. We can verify that applying the two functions successively returns the original parameter values:
```{r norm-test}
# List of parameters (on the natural scale)
par1 <- list(mean = c(0, 5), sd = c(1, 10))

# Transform to working scale
wpar <- dist_norm$n2w(par1)
wpar

# Transform back to natural scale
par2 <- dist_norm$w2n(wpar)
par2
```

## HMM data

Data objects are defined with the HmmData class. An object can be created from a data frame which includes one column for each observed variable and for each covariate. There are special column names: "ID" is used to identify the time series if several are provided, and "time" gives the times of observation. If the times of observations are provided, gaps are automatically filled with NAs to create a obtain a regular time grid.

```{r create-data}
# Create a dummy data set, with two time series
times1 <- seq(as.POSIXct("2020/01/01 00:00:00"), by = "hour", length = 10)
times2 <- seq(as.POSIXct("2020/02/01 00:00:00"), by = "hour", length = 10)
data <- data.frame(ID = rep(c(1, 2), each = 10),
                   x = rnorm(20), 
                   time = c(times1, times2))

# Remove a few rows to create gaps in the data
data <- data[-c(2:3, 7, 15:18),]

# Define HmmData object
data_prep <- HmmData$new(data = data, interval = "hour")

# The gaps have been filled
data_prep$data()
```

## HMM observation model

The class Observation encapsulates the model for the observed variables.

```{r obs-create}
# Create a dummy data set
times <- seq(as.POSIXct("2020/01/01 00:00:00"), by = "hour", length = 100)
data <- data.frame(ID = rep(1, 100),
                   x = rnorm(100), 
                   time = times)
data_prep <- HmmData$new(data = data)

# List of observation distributions
dists <- list(x = dist_norm)

# List of observation parameters
par <- list(x = list(mean = c(0, 0), sd = c(1, 10)))

# Number of states
n_states <- 2

# Create observation process object
obs_process <- Observation$new(data = data_prep, dists = dists, 
                               n_states = n_states, par = par)
```

The parameters on the working scale are calculated when the object is created:
```{r obs-wpar}
obs_process$coeff_fe()
```

The function plot_dist generates a histogram of the observations with the probability density (or mass) function of the specified distribution.
```{r obs-plot}
# Plot histogram and pdf for variable "x"
obs_process$plot_dist(name = "x")
```

# How to extend the package

  - How to add distributions

# Future work

## Higher priority

 - prepare package for sharing:
   + uniformize names/notation/..., e.g., captilization of class names
   + improve class structure, e.g., groups of methods
   + remove HmmData class if useless, or allow for data frame to be passed instead
   + add comprehensive example(s) to vignette
   + possibly split vignette into technical guide (for other devs) and vignette (for users)
   + check that the code is commented well
   + check that the documentation is adequate
   + reduce code redundancy where possible (e.g., make_mat)

 - additional observation distributions
    + Weibull
    + log-normal
    + wrapped Cauchy
    + Bernoulli...
    
## Lower priority (easy?)

 - state probabilities (local decoding)

 - pseudo-residuals

 - allow for input of known states (semi-supervised learning)

 - deal with NAs in covariates (linear interpolation? nearest value? error message?)

 - stationary = TRUE/FALSE option in MarkovChain class
 
## Lower priority (difficult?)

 - AIC
   
 - zero inflation
 
 - improve parametrisation of mean of circular distributions (avoid numerical issues around -pi and pi)
 
 - allow for constraints on parameters
    + inequalities, e.g. $\mu_1 < \mu_2$
    + bounds, e.g. $-1 < \mu_1 < 1$ (using custom link function)
    + fixed value, e.g. $\mu_1 = 0$ (using map)

 - obtain starting values for complex models from simpler nested model (e.g. for smooth effects)

 - method "update" similar to glmmTMB, to create a new model from an existing model?
 
# References